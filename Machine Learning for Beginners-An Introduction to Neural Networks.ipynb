{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从头实现一个神经网络\n",
    "神经网络其实就是把多个神经元连在一起，形成一个网络。上一层神经元的输出作为下一层神经元的输入。\n",
    "下图就是一个神经网络示意图，红色和蓝色的节点就是神经元，这是一个简单的神经网络，复杂的网络无非就是输入多一些，隐层多一些而已，本质上还是这样的。\n",
    "<img src=\"https://raw.githubusercontent.com/fioccy/pics/master/Machine%20Learning%20for%20Beginners-An%20Introduction%20to%20Neural%20Networks/20190902170213.png\" width=\"500\" hegiht=\"313\" align=center/>\n",
    "\n",
    "## 1.神经元\n",
    "神经元是神经网络的基本单元，一个神经元可以理解为对所有的输入值$(x)$加权$(w)$，然后相加，再加上偏置($\\theta$),得到这一组输入的计算值，再将计算值代入到激活函数中，得到这个神经元的输出。计算值可以理解为神经元的输入所产生的刺激的强度，神经元的输出可以理解为神经元对于这个强度的刺激是否作出反应。\n",
    "<img src=\"https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=27a32abe59fbb2fb202650402e234bc1/8cb1cb1349540923a319419d9e58d109b2de49e4.jpg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "最原始的激活函数是阶跃函数，其函数图像如下：\n",
    "<img src=\"https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=3241984634,2952446636&fm=26&gp=0.jpg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "这与人类的神经元的工作方式类似，即当刺激强度足够的时候，我就作出响应（输出1），当刺激强度不够的时候，我就不响应（输出0）。\n",
    "但是这个函数不是连续可导的，现在常用的激活函数是**Sigmoid函数**，其函数图像如下所示：\n",
    "<img src=\"https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike116%2C5%2C5%2C116%2C38/sign=662513026ed9f2d3341c2cbdc885e176/730e0cf3d7ca7bcb1797ebccb2096b63f724a860.jpg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "Sigmoid函数的优点有单增，连续可导，其导数也单增等，因此十分适合用来做激活函数。\n",
    "除了Sigmoid函数，其他常用的激活函数还有：\n",
    "\n",
    "**Tanh函数** \n",
    "<img src=\"https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike72%2C5%2C5%2C72%2C24/sign=5484883dfc1f3a294ec5dd9cf84cd754/b64543a98226cffcc6c79651b5014a90f703ea60.jpg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "\n",
    "**ReLU函数** \n",
    "<img src=\"https://gss0.bdstatic.com/-4o3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=9df52608bafd5266b3263446ca71fc4e/f31fbe096b63f624775e13e08b44ebf81b4ca3d5.jpg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "为了方便，我们以拥有两个输入的神经元为例进行讲解。\n",
    "<img src=\"https://github.com/fioccy/pics/raw/master/Machine%20Learning%20for%20Beginners-An%20Introduction%20to%20Neural%20Networks/20190902204119.png\" width=\"500\" hegiht=\"313\" align=center />\n",
    "如图所示，这是一个拥有两个输入$x_1$和$x_2$的神经元，每个输入对应的权重为$\\omega_1$和$\\omega_2$，偏置设为$b$，则这个神经元对输入的计算值为\n",
    "$$(x_1\\times\\omega_1)+(x_2\\times\\omega_2)+b$$\n",
    "将上式代入激活函数得到该神经元的输出：\n",
    "$$y = f((x_1\\times\\omega_1)+(x_2\\times\\omega_2)+b)$$\n",
    "假设该神经元的两个权重$\\omega_1 = 0$，$\\omega_2 = 1$，偏置$b = 4$，输入$x_1 = 2$,$x_2 = 3$,将上述值代入神经元公式，得到计算值为：\n",
    "$$ \\begin{align*} \\omega_1\\times x_1+\\omega_2\\times x_2+b &= 0\\times2 +1\\times3 +4\\\\&= 7\\end{align*}$$\n",
    "将计算值7代入激活函数Sigmoid，$Sigmoid(7)=\\boxed{0.99}$。也就是说这个神经元对这一组输入$\\begin{bmatrix}2&3\\end{bmatrix}$的输出是0.99。**这种给定输入，得到输出的过程被称之为前馈。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码实现一个神经元\n",
    "使用NumPy库来帮助实现神经元。通过上边的描述可以知道，一个神经元干的事情就是对输入加权，然后加上偏置，将此计算值代入激活函数得到最终的神经元输出。输入是由外部提供的，那么神经元自身的属性其实就是权重和偏置，神经元的函数（即神经元功能）即是实现上述的计算过程，得到输出。\n",
    "通过定义一个神经元类，将神经元的权重和偏置作为类的属性，生成一个具体的神经元对象的时候通过初始化函数指定该神经元的权重和偏置的具体值。还定义了前馈的具体实现，即将输入乘以权重之后相加，再加上偏置，得到计算值，将此计算值代入激活函数sigmoid，得到神经元的输出。一个基本的神经元大概就包含这些内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# sigmoid激活函数\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# 神经元类\n",
    "class Neuron():\n",
    "#     初始化，一个神经元其实就两个参数，每个输入的权重和偏置，这是为了方便理解故意写的比较原始\n",
    "#其实偏置也可以放到权重向量里，通常记为 w_0，它的输入为 x_0 = 1，这样的话神经元就只有一个参数-权重。\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    \n",
    "    #定义了神经元的前馈函数，即将输入与权重相乘再加上偏置，得到的计算值再代入激活函数中，得到神经元的最终输出    \n",
    "    def feedforward(self, inputs):\n",
    "        #这步先计算神经元的输入的计算值，利用numpy的向量点积进行计算\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "       #代入到激活函数sigmoid中，得到神经元的输出值\n",
    "        return sigmoid(total)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#用np库的array函数生成向量 \n",
    "#神经元的权重，这里只设置了2个权重，2个输入\n",
    "    weights = np.array([0,1])\n",
    "    bias = 4  #偏置\n",
    "    \n",
    "    myNeuron = Neuron(weights, bias)\n",
    "    x = np.array([2,3])  #两个输入\n",
    "    neuron_output = myNeuron.feedforward(x)\n",
    "    print(neuron_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.将神经元组成神经网络\n",
    "神经网络就是将神经元彼此连接，上一层神经元的输出作为下一层神经元的输入。还是以有两个输入的神经元为例进行说明。下图展示了一个最简单的神经网络的构成。这个神经网络有两个输入，一个隐层包含2个神经元（$h_1$和$h_2$），一个输出层，包含1个神经元（$o_1$）。$h_1$和$h_2$的输出是$o_1$的输入。\n",
    "> 隐层是指输入和输出之间的层，可以有很多层。所谓深度学习就是隐层数很多的神经网络\n",
    "\n",
    "<img src=\"https://victorzhou.com/network-77ed172fdef54ca1ffcfb0bba27ba334.svg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "\n",
    "### 前馈\n",
    "现在假设每个神经元的权重均为$\\omega=[0,1]$，偏置$bias$均为0，且激活函数都为sigmoid函数。那么如果输入$x=[2,3]$，整个神经网络的输出会是多少？\n",
    "\n",
    "+ 先算$h_1$和$h_2$：\n",
    "$$ \\begin{align*} h_1= h_2  &= sigmoid(0\\times2 +1\\times3 +0)\\\\&= sigmoid(3)\\\\&=0.9526\\end{align*}$$\n",
    "+ 再算$o_1$：\n",
    "$$ \\begin{align*} o_1  &= sigmoid(0\\times 0.9526 +1\\times0.9526 +0)\\\\&= sigmoid(0.9526)\\\\&=\\boxed{0.7216}\\end{align*}$$\n",
    "\n",
    "可以看到对于输入$x=[2,3]$，整个神经网络的输出为0.7216\n",
    "一个神经网络可以有任意多个层和任意多个神经元，但是其工作的基本流程都是一样的，一层一层的算，上一层的输出作为下一层的输入，最终得到整个神经网络的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码实现一个神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "因为这个cell里调用了上一个cell定义的Neuron类且使用的NumPy库也是在上一个cell里导入的\n",
    "因此如果上边的cell已经运行过了，则可以直接运行该cell，如果这次打开这个notebook以来，上一个cell还没运行过，则需要先运行上边的cell才能运行该cell\n",
    "'''\n",
    "#神经网络的类\n",
    "class OurNeuralNetwork():\n",
    "    #初始化函数，神经网络是由神经元组成的，因此初始化神经网络就是要初始化其中的神经元\n",
    "    #根据假设，该神经网络由h1，h2和o1共3个神经元组成，每个神经元的权重均为w1=0,w2=1，bias=0\n",
    "    #这里调用了上一个cell里定义的Neuron类来初始化神经元\n",
    "    def __init__(self):\n",
    "        weights = np.array([0,1])\n",
    "        bias = 0\n",
    "        \n",
    "        self.h1 = Neuron(weights,bias)\n",
    "        self.h2 = Neuron(weights,bias)\n",
    "        self.o1 = Neuron(weights,bias)\n",
    "    \n",
    "    #定义这个神经网络的前馈，其实就是一层层求值的过程。先计算h1和h2的输出值，然后将h1和h2的输出作为o1的输入，计算得到网络最终的输出值\n",
    "    def feedforward(self,x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "\n",
    "#生成一个实例\n",
    "network = OurNeuralNetwork()\n",
    "# 设置网络的输入\n",
    "x = np.array([2,3])\n",
    "# 调用前馈函数得到网络的输出\n",
    "print(network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到程序输出也是0.7216，与我们之前的推导是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.训练神经网络（第一部分）\n",
    "如果我们有如下的输入：\n",
    "\n",
    "Name|Weight|Height|Gender\n",
    "-|-|-|-\n",
    "Alice|133|65|F\n",
    "Bob|160|72|M\n",
    "Charlie|152|70|M\n",
    "Diana|120|60|F\n",
    "\n",
    "现在希望通过身高和体重来预测性别。还是使用上边的那个有两个输入，一个隐层，一个输出的神经网络。使用0表示Male，1表示Female，并对表中的数据进行归一处理。得到如下的数据表：\n",
    "\n",
    "|Name|Weight(减去135)|Height(减去66)|Gender|\n",
    "|-|:-:|:-:|:-:|\n",
    "|Alice|-2|-1|1|\n",
    "|Bob|25|6|0|\n",
    "|Charlie|17|4|0|\n",
    "|Diana|-15|-6|1|\n",
    "\n",
    "### 损失函数\n",
    "使用神经网络进行预测，需要衡量其预测的好不好，一般会定义一个**损失函数**来对其进行度量。这里我们选用均方差（MSE）作为损失函数：\n",
    "$$MSE=\\frac1n\\sum_{i=1}^n{(y_{true}-y_{pred})^2}\\tag{3.1}$$\n",
    "其中：\n",
    "+ $n$代表样本的个数，即为4\n",
    "+ $y$代表被预测的值，即为性别\n",
    "+ $y_{true}$是样本真实值\n",
    "+ $y_{pred}$是神经网络计算输出值\n",
    "\n",
    "好的网络就是预测值与真实值最接近的网络，也就是损失函数（均方差）最小的网络。因此：\n",
    "**所谓训练网络，其实就是调整网络的参数，使其损失函数最小**\n",
    "### 计算损失函数的例子\n",
    "假设我们的网络永远输出0，那么对于上边的数据，其损失函数是多少？\n",
    "\n",
    "|Name|$y_{true}$|$y_{pred}$|$(y_{true}-y_{pred})^2$|\n",
    "|-|:-:|:-:|:-:|\n",
    "|Alice|1|0|1|\n",
    "|Bob|0|0|0|\n",
    "|Charlie|0|0|0|\n",
    "|Diana|1|0|1|\n",
    "\n",
    "根据表格，代入MSE公式，可计算得到：\n",
    "$$MSE=\\frac{1}{4}(1+0+0+1)=\\boxed{0.5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码实现损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred)**2).mean()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    y_true=np.array([1,0,0,1])\n",
    "    y_pred=np.array([0,0,0,0])\n",
    "    print(mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.训练神经网络（第二部分）\n",
    "训练神经网络就是调整网络的参数，使其预测值最接近实际值，即最小化损失函数。但是问题是，该如何调整参数？\n",
    "为了简单起见，先假设数据集中只有一个样本Alice：\n",
    "\n",
    "Name|Weight|Height|Gender\n",
    "-|-|-|-\n",
    "Alice|-2|-1|1\n",
    "\n",
    "这种情况下，网络的误差为：\n",
    "$$\n",
    "\\begin{split}L=MSE &= \\frac{1}{1}\\sum_{i=1}^1{(y_{true}-y_{pred})^2}\\\\\n",
    "&=(y_{true}-y_{pred})^2\\\\\n",
    "&=(1-y_{pred})^2\n",
    "\\end{split} \\tag{4.1}\n",
    "$$\n",
    "\n",
    "将网络的权重和偏置都标记出来，网络图是这样的：\n",
    "<img src=\"https://victorzhou.com/network3-27cf280166d7159c0465a58c68f99b39.svg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "从上边的推导可以看出，网络的损失(MSE)是$y_{pred}$的函数，由网络的定义可知，$y_{pred}$又可以看做是输入权重$\\omega_1, \\omega_2,\\omega_3，\\omega_4，\\omega_5，\\omega_6$和偏置$b_1,b_2,b_3$的函数。\n",
    "**这里要转换一下思路，当权重和偏置未改变的时候，网络的变量是输入$x$，给不同的输入，网络输出不同的值。而现在我们要考虑的是改变权重和偏置，使得网络在同样的输入$x$的情况下，输出值变化（更接近真实值），也就是说此时，我们应该将网络的损失函数看做是权重和偏置的函数，记为：**\n",
    "\n",
    "$$L(\\omega_1,\\omega_2,\\omega_3,\\omega_4,\\omega_5,\\omega_6,b_1,b_2,b_3)$$\n",
    "\n",
    "假设我们要通过改变$\\omega_1$来使得网络损失函数$L()$变化，那么该如何改变它的值呢？显然不能乱变，我们的目的是降低网络的损失，也就是说我们希望通过改变$\\omega_1$来减小MSE，这里自然就会用到**梯度**的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "关于梯度的理解和学习，可以参考[深入浅出--梯度下降法及其实现](https://www.jianshu.com/p/c7e642877b0e)这篇文章。基本上已经讲的很明白了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过计算损失函数$L$对$\\omega_1$的偏导，来指导进行对$\\omega_1$的微调。根据求导公式可以得出：\n",
    "$$\\frac{\\partial L}{\\partial\\omega_1}=\\frac{\\partial L}{\\partial y_{pred}}\\times\\frac{\\partial y_{pred}}{\\partial \\omega_1}\\tag{4.2}$$\n",
    "其中，由公式4.1，可以得出：\n",
    "$$\\frac{\\partial L}{\\partial y_{pred}}=\\boxed{-2\\times(1-y_{pred})} \\tag{4.3}$$\n",
    "为了方便表示，对于神经元$a$，记其输入乘以加权再加上偏置为$V_a$，即$V_{o_1}=\\omega_5\\times h_1+\\omega_6\\times h_2+b_3$,再由神经网络的计算过程可以得出$y_{pred}=Sigmoid(V_{o_1})$，所以：\n",
    "$$\\frac{\\partial y_{pred}}{\\partial \\omega_1}=\\frac{\\partial S}{\\partial V_{o_1}}\\times\\frac{\\partial V_{o_1}}{\\partial \\omega_1} \\tag{4.4}$$\n",
    "已知$Sigmoid$函数的导数形式为$S'(x)=S(x)\\times(1-S(x))$，所以\n",
    "$$\\frac{\\partial S}{\\partial V_{o_1}}={S(V_{o_1})\\times(1-S(V_{o_1}))}=\\boxed{o_1\\times(1-o_1)}\\tag{4.5}$$\n",
    "现在来看$\\frac{\\partial V_{o_1}}{\\partial \\omega_1}$，由神经网络计算过程可知，只有$h_1$与$\\omega_1$有关，因此\n",
    "$$\\begin{split}\\frac{\\partial V_{o_1}}{\\partial \\omega_1}&=\\frac{\\partial V_{o_1}}{\\partial h_1}\\times\\frac{\\partial h_1}{\\partial \\omega_1}\\end{split}$$\n",
    "已知$V_{o_1}=\\omega_5\\times h_1+\\omega_6\\times h_2+b_3$，因此\n",
    "$$\\frac{\\partial V_{o_1}}{\\partial h_1} = \\boxed{\\omega_5}\\tag{4.6}$$\n",
    "已知$h_1=S(V_{h_1})=S(\\omega_1\\times x_1+\\omega_2\\times x_2+b_1)$，因此\n",
    "$$\\begin{split}\\frac{\\partial h_1}{\\partial \\omega_1} &= \\frac{\\partial S}{\\partial V_{h_1}}\\times\\frac{\\partial V_{h_1}}{\\omega_1}\\\\\n",
    "&= {S(V_{h_1})\\times(1-S(V_{h_1}))\\times x_1}\\\\\n",
    "&=\\boxed{h_1\\times(1-h_1)\\times x_1}\\end{split}\\tag{4.7}$$\n",
    "这种通过向后计算来求偏导的方式就是传说中的**反向传播** ,通过上边的推导，我们就把损失函数$L$对$\\omega_1$的偏导分解成了几个可以计算的部分（上边公式中框起来的部分）。下边实际计算一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实际计算一下偏导\n",
    "继续假设数据中只有Alice一个样本。\n",
    "\n",
    "|Name|Weight|Height|Gender|\n",
    "|-|:-:|:-:|:-:|\n",
    "|Alice|-2|-1|1\n",
    "\n",
    "先假设所有的权重均为1，所有的偏置均为0，先计算一下网络的输出：\n",
    "$$\\begin{split}h_1&=S(\\omega_1\\times x_1+\\omega_2\\times x_2+b_1)\\\\\n",
    "&=S(-2-1+0)\\\\\n",
    "&=0.0474\\end{split}\n",
    "$$\n",
    "\n",
    "$h_2$的输入、权重与偏置均与$h_1$一致，因此$h_2=0.0474$。下边计算$o_1$:\n",
    "$$\\begin{split}o_1&=S(\\omega_5\\times h_1+\\omega_6\\times h_2+b_3)\\\\\n",
    "&=S(0.0474+0.0474)\\\\\n",
    "&=0.524\\end{split}\n",
    "$$\n",
    "神经网络输出的结果为0.524，对男(0)或女(1)没有很强的指向性。\n",
    "现在开始计算$\\frac{\\partial L}{\\partial \\omega_1}$。根据上边的推导\n",
    "$$\\begin{split}\\frac{\\partial L}{\\partial y_{pred}}&=-2\\times(1-y_{pred})\\\\\n",
    "&=-2\\times(1-0.524)\\\\\n",
    "&=\\boxed {-0.952}\n",
    "\\end{split}$$\n",
    "\n",
    "$$\\begin{split}\\frac{\\partial S}{\\partial o_{1}}&=o_1\\times(1-o_1)\\\\\n",
    "&=0.524\\times(1-0.524)\\\\\n",
    "&=\\boxed {0.249424}\n",
    "\\end{split}$$\n",
    "\n",
    "$$\\begin{split}\\frac{\\partial V_{o_1}}{\\partial h_{1}}&=\\omega_5\\\\\n",
    "&=\\boxed 1\n",
    "\\end{split}$$\n",
    "\n",
    "$$\\begin{split}\\frac{\\partial h_1}{\\partial \\omega_{1}}&=h_1\\times(1-h_1)\\times x_1\\\\\n",
    "&=0.0474\\times(1-0.0474)\\times-2\\\\\n",
    "&=\\boxed {-0.09030648}\n",
    "\\end{split}$$\n",
    "所以，最终得到的$\\frac{\\partial L}{\\partial \\omega_1}$的值为$-0.952\\times0.249424\\times1\\times-0.09030648=0.02144$\n",
    "损失函数$L$对$\\omega_1$的偏导为0.02144，意味着$\\omega_1$的增加会导致$L$也随之增加（一点点点点）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降（Stochastic Gradient Descent）\n",
    "随机梯度下降法之所以带“随机”两个字，是因为**随机梯度下降**的工作方式是代入一个样本得到神经网络输出值之后，就利用反向传播对网络参数进行修正，然后再利用修正后的参数去计算下一个样本，以此类推。而**梯度下降**的工作方式则是将所有样本都训练完成后，根据公式（3.1）计算网络误差并按照反向传播的思路去更新网络参数，在梯度下降方法中，数据集中的样本全部算完一次之后再更新网络，每个样本都对更新参数做了贡献。  \n",
    "总结来说就是： <br/>\n",
    "**随机梯度下降**：计算完一个样本更新一次网络 <br/>\n",
    "**梯度下降**：计算完数据集里的所有样本后，更新一次网络 <br/>\n",
    "这两种方法各有利弊，随机梯度下降计算量小，但是每次更新的方向不是特别准确，需要的迭代次数多（理论上证明随机梯度下降是能够找到最小值的），而梯度下降则方向准确，但计算量大。在神经网络中，一般使用随机梯度下降法。现在我们使用随机梯度下降法（SGD）来对网络进行优化。  \n",
    "优化的基本公式是：  \n",
    "$$\n",
    "\\omega_1\\leftarrow\\omega_1-\\eta\\frac{\\partial L}{\\partial \\omega_1}\n",
    "$$\n",
    "其中，$\\eta$被称为**学习率**（其实就是在进行梯度下降的时候，每一步的步长）。学习率是机器学习中一个非常常见和重要的一个超参。学习率过大会导致可能错过极值点，学习率过小则会导致迭代速度过慢。  \n",
    "在上边的公式中，如果偏导为正（即$L$与$\\omega_1$正相关），则通过公式，$\\omega_1$减小，$L$也随之减小，如果偏导为负(即$L$与$\\omega_1$正相关)，则通过公式，$\\omega_1$增加，同样会导致$L$减小。\n",
    "\n",
    "如果对每个参数和偏置均使用上边的公式进行修正，则损失函数会逐步下降，网络表现也会越来越好。  \n",
    "\n",
    "因此训练网络的过程即为：  \n",
    "1. 从数据中选取一个样本\n",
    "2. 计算损失函数对每个参数和偏置的偏导（$\\frac{\\partial L}{\\partial \\omega_1}$、$\\frac{\\partial L}{\\partial \\omega_2}$等等）\n",
    "3. 利用更新公式更新网络参数\n",
    "4. 重复上述步骤\n",
    "\n",
    "现在来编码实现这个神经网络。\n",
    "网络图如下：\n",
    "<img src=\"https://victorzhou.com/network3-27cf280166d7159c0465a58c68f99b39.svg\" width=\"500\" hegiht=\"313\" align=center />\n",
    "网络输入如下：\n",
    "\n",
    "|Name|Weight(减去135)|Height(减去66)|Gender|\n",
    "|-|:-:|:-:|:-:|\n",
    "|Alice|-2|-1|1|\n",
    "|Bob|25|6|0|\n",
    "|Charlie|17|4|0|\n",
    "|Diana|-15|-6|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.483\n",
      "Epoch 0 loss: 0.480\n",
      "Epoch 0 loss: 0.477\n",
      "Epoch 0 loss: 0.476\n",
      "Epoch 10 loss: 0.356\n",
      "Epoch 10 loss: 0.351\n",
      "Epoch 10 loss: 0.345\n",
      "Epoch 10 loss: 0.345\n",
      "Epoch 20 loss: 0.235\n",
      "Epoch 20 loss: 0.230\n",
      "Epoch 20 loss: 0.225\n",
      "Epoch 20 loss: 0.224\n",
      "Epoch 30 loss: 0.148\n",
      "Epoch 30 loss: 0.146\n",
      "Epoch 30 loss: 0.144\n",
      "Epoch 30 loss: 0.143\n",
      "Epoch 40 loss: 0.101\n",
      "Epoch 40 loss: 0.100\n",
      "Epoch 40 loss: 0.099\n",
      "Epoch 40 loss: 0.099\n",
      "Epoch 50 loss: 0.073\n",
      "Epoch 50 loss: 0.073\n",
      "Epoch 50 loss: 0.073\n",
      "Epoch 50 loss: 0.072\n",
      "Epoch 60 loss: 0.056\n",
      "Epoch 60 loss: 0.056\n",
      "Epoch 60 loss: 0.055\n",
      "Epoch 60 loss: 0.055\n",
      "Epoch 70 loss: 0.044\n",
      "Epoch 70 loss: 0.044\n",
      "Epoch 70 loss: 0.044\n",
      "Epoch 70 loss: 0.044\n",
      "Epoch 80 loss: 0.036\n",
      "Epoch 80 loss: 0.036\n",
      "Epoch 80 loss: 0.036\n",
      "Epoch 80 loss: 0.036\n",
      "Epoch 90 loss: 0.031\n",
      "Epoch 90 loss: 0.031\n",
      "Epoch 90 loss: 0.030\n",
      "Epoch 90 loss: 0.030\n",
      "Epoch 100 loss: 0.026\n",
      "Epoch 100 loss: 0.026\n",
      "Epoch 100 loss: 0.026\n",
      "Epoch 100 loss: 0.026\n",
      "Epoch 110 loss: 0.023\n",
      "Epoch 110 loss: 0.023\n",
      "Epoch 110 loss: 0.023\n",
      "Epoch 110 loss: 0.023\n",
      "Epoch 120 loss: 0.020\n",
      "Epoch 120 loss: 0.020\n",
      "Epoch 120 loss: 0.020\n",
      "Epoch 120 loss: 0.020\n",
      "Epoch 130 loss: 0.018\n",
      "Epoch 130 loss: 0.018\n",
      "Epoch 130 loss: 0.018\n",
      "Epoch 130 loss: 0.018\n",
      "Epoch 140 loss: 0.016\n",
      "Epoch 140 loss: 0.016\n",
      "Epoch 140 loss: 0.016\n",
      "Epoch 140 loss: 0.016\n",
      "Epoch 150 loss: 0.015\n",
      "Epoch 150 loss: 0.015\n",
      "Epoch 150 loss: 0.015\n",
      "Epoch 150 loss: 0.015\n",
      "Epoch 160 loss: 0.014\n",
      "Epoch 160 loss: 0.014\n",
      "Epoch 160 loss: 0.014\n",
      "Epoch 160 loss: 0.014\n",
      "Epoch 170 loss: 0.013\n",
      "Epoch 170 loss: 0.013\n",
      "Epoch 170 loss: 0.013\n",
      "Epoch 170 loss: 0.013\n",
      "Epoch 180 loss: 0.012\n",
      "Epoch 180 loss: 0.012\n",
      "Epoch 180 loss: 0.012\n",
      "Epoch 180 loss: 0.012\n",
      "Epoch 190 loss: 0.011\n",
      "Epoch 190 loss: 0.011\n",
      "Epoch 190 loss: 0.011\n",
      "Epoch 190 loss: 0.011\n",
      "Epoch 200 loss: 0.010\n",
      "Epoch 200 loss: 0.010\n",
      "Epoch 200 loss: 0.010\n",
      "Epoch 200 loss: 0.010\n",
      "Epoch 210 loss: 0.010\n",
      "Epoch 210 loss: 0.010\n",
      "Epoch 210 loss: 0.010\n",
      "Epoch 210 loss: 0.009\n",
      "Epoch 220 loss: 0.009\n",
      "Epoch 220 loss: 0.009\n",
      "Epoch 220 loss: 0.009\n",
      "Epoch 220 loss: 0.009\n",
      "Epoch 230 loss: 0.008\n",
      "Epoch 230 loss: 0.008\n",
      "Epoch 230 loss: 0.008\n",
      "Epoch 230 loss: 0.008\n",
      "Epoch 240 loss: 0.008\n",
      "Epoch 240 loss: 0.008\n",
      "Epoch 240 loss: 0.008\n",
      "Epoch 240 loss: 0.008\n",
      "Epoch 250 loss: 0.008\n",
      "Epoch 250 loss: 0.008\n",
      "Epoch 250 loss: 0.008\n",
      "Epoch 250 loss: 0.008\n",
      "Epoch 260 loss: 0.007\n",
      "Epoch 260 loss: 0.007\n",
      "Epoch 260 loss: 0.007\n",
      "Epoch 260 loss: 0.007\n",
      "Epoch 270 loss: 0.007\n",
      "Epoch 270 loss: 0.007\n",
      "Epoch 270 loss: 0.007\n",
      "Epoch 270 loss: 0.007\n",
      "Epoch 280 loss: 0.007\n",
      "Epoch 280 loss: 0.007\n",
      "Epoch 280 loss: 0.007\n",
      "Epoch 280 loss: 0.007\n",
      "Epoch 290 loss: 0.006\n",
      "Epoch 290 loss: 0.006\n",
      "Epoch 290 loss: 0.006\n",
      "Epoch 290 loss: 0.006\n",
      "Epoch 300 loss: 0.006\n",
      "Epoch 300 loss: 0.006\n",
      "Epoch 300 loss: 0.006\n",
      "Epoch 300 loss: 0.006\n",
      "Epoch 310 loss: 0.006\n",
      "Epoch 310 loss: 0.006\n",
      "Epoch 310 loss: 0.006\n",
      "Epoch 310 loss: 0.006\n",
      "Epoch 320 loss: 0.006\n",
      "Epoch 320 loss: 0.006\n",
      "Epoch 320 loss: 0.006\n",
      "Epoch 320 loss: 0.006\n",
      "Epoch 330 loss: 0.005\n",
      "Epoch 330 loss: 0.005\n",
      "Epoch 330 loss: 0.005\n",
      "Epoch 330 loss: 0.005\n",
      "Epoch 340 loss: 0.005\n",
      "Epoch 340 loss: 0.005\n",
      "Epoch 340 loss: 0.005\n",
      "Epoch 340 loss: 0.005\n",
      "Epoch 350 loss: 0.005\n",
      "Epoch 350 loss: 0.005\n",
      "Epoch 350 loss: 0.005\n",
      "Epoch 350 loss: 0.005\n",
      "Epoch 360 loss: 0.005\n",
      "Epoch 360 loss: 0.005\n",
      "Epoch 360 loss: 0.005\n",
      "Epoch 360 loss: 0.005\n",
      "Epoch 370 loss: 0.005\n",
      "Epoch 370 loss: 0.005\n",
      "Epoch 370 loss: 0.005\n",
      "Epoch 370 loss: 0.005\n",
      "Epoch 380 loss: 0.005\n",
      "Epoch 380 loss: 0.005\n",
      "Epoch 380 loss: 0.005\n",
      "Epoch 380 loss: 0.005\n",
      "Epoch 390 loss: 0.004\n",
      "Epoch 390 loss: 0.004\n",
      "Epoch 390 loss: 0.004\n",
      "Epoch 390 loss: 0.004\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 410 loss: 0.004\n",
      "Epoch 410 loss: 0.004\n",
      "Epoch 410 loss: 0.004\n",
      "Epoch 410 loss: 0.004\n",
      "Epoch 420 loss: 0.004\n",
      "Epoch 420 loss: 0.004\n",
      "Epoch 420 loss: 0.004\n",
      "Epoch 420 loss: 0.004\n",
      "Epoch 430 loss: 0.004\n",
      "Epoch 430 loss: 0.004\n",
      "Epoch 430 loss: 0.004\n",
      "Epoch 430 loss: 0.004\n",
      "Epoch 440 loss: 0.004\n",
      "Epoch 440 loss: 0.004\n",
      "Epoch 440 loss: 0.004\n",
      "Epoch 440 loss: 0.004\n",
      "Epoch 450 loss: 0.004\n",
      "Epoch 450 loss: 0.004\n",
      "Epoch 450 loss: 0.004\n",
      "Epoch 450 loss: 0.004\n",
      "Epoch 460 loss: 0.004\n",
      "Epoch 460 loss: 0.004\n",
      "Epoch 460 loss: 0.004\n",
      "Epoch 460 loss: 0.004\n",
      "Epoch 470 loss: 0.004\n",
      "Epoch 470 loss: 0.004\n",
      "Epoch 470 loss: 0.004\n",
      "Epoch 470 loss: 0.004\n",
      "Epoch 480 loss: 0.003\n",
      "Epoch 480 loss: 0.003\n",
      "Epoch 480 loss: 0.003\n",
      "Epoch 480 loss: 0.003\n",
      "Epoch 490 loss: 0.003\n",
      "Epoch 490 loss: 0.003\n",
      "Epoch 490 loss: 0.003\n",
      "Epoch 490 loss: 0.003\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 510 loss: 0.003\n",
      "Epoch 510 loss: 0.003\n",
      "Epoch 510 loss: 0.003\n",
      "Epoch 510 loss: 0.003\n",
      "Epoch 520 loss: 0.003\n",
      "Epoch 520 loss: 0.003\n",
      "Epoch 520 loss: 0.003\n",
      "Epoch 520 loss: 0.003\n",
      "Epoch 530 loss: 0.003\n",
      "Epoch 530 loss: 0.003\n",
      "Epoch 530 loss: 0.003\n",
      "Epoch 530 loss: 0.003\n",
      "Epoch 540 loss: 0.003\n",
      "Epoch 540 loss: 0.003\n",
      "Epoch 540 loss: 0.003\n",
      "Epoch 540 loss: 0.003\n",
      "Epoch 550 loss: 0.003\n",
      "Epoch 550 loss: 0.003\n",
      "Epoch 550 loss: 0.003\n",
      "Epoch 550 loss: 0.003\n",
      "Epoch 560 loss: 0.003\n",
      "Epoch 560 loss: 0.003\n",
      "Epoch 560 loss: 0.003\n",
      "Epoch 560 loss: 0.003\n",
      "Epoch 570 loss: 0.003\n",
      "Epoch 570 loss: 0.003\n",
      "Epoch 570 loss: 0.003\n",
      "Epoch 570 loss: 0.003\n",
      "Epoch 580 loss: 0.003\n",
      "Epoch 580 loss: 0.003\n",
      "Epoch 580 loss: 0.003\n",
      "Epoch 580 loss: 0.003\n",
      "Epoch 590 loss: 0.003\n",
      "Epoch 590 loss: 0.003\n",
      "Epoch 590 loss: 0.003\n",
      "Epoch 590 loss: 0.003\n",
      "Epoch 600 loss: 0.003\n",
      "Epoch 600 loss: 0.003\n",
      "Epoch 600 loss: 0.003\n",
      "Epoch 600 loss: 0.003\n",
      "Epoch 610 loss: 0.003\n",
      "Epoch 610 loss: 0.003\n",
      "Epoch 610 loss: 0.003\n",
      "Epoch 610 loss: 0.003\n",
      "Epoch 620 loss: 0.003\n",
      "Epoch 620 loss: 0.003\n",
      "Epoch 620 loss: 0.003\n",
      "Epoch 620 loss: 0.003\n",
      "Epoch 630 loss: 0.003\n",
      "Epoch 630 loss: 0.003\n",
      "Epoch 630 loss: 0.003\n",
      "Epoch 630 loss: 0.003\n",
      "Epoch 640 loss: 0.002\n",
      "Epoch 640 loss: 0.002\n",
      "Epoch 640 loss: 0.002\n",
      "Epoch 640 loss: 0.002\n",
      "Epoch 650 loss: 0.002\n",
      "Epoch 650 loss: 0.002\n",
      "Epoch 650 loss: 0.002\n",
      "Epoch 650 loss: 0.002\n",
      "Epoch 660 loss: 0.002\n",
      "Epoch 660 loss: 0.002\n",
      "Epoch 660 loss: 0.002\n",
      "Epoch 660 loss: 0.002\n",
      "Epoch 670 loss: 0.002\n",
      "Epoch 670 loss: 0.002\n",
      "Epoch 670 loss: 0.002\n",
      "Epoch 670 loss: 0.002\n",
      "Epoch 680 loss: 0.002\n",
      "Epoch 680 loss: 0.002\n",
      "Epoch 680 loss: 0.002\n",
      "Epoch 680 loss: 0.002\n",
      "Epoch 690 loss: 0.002\n",
      "Epoch 690 loss: 0.002\n",
      "Epoch 690 loss: 0.002\n",
      "Epoch 690 loss: 0.002\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 710 loss: 0.002\n",
      "Epoch 710 loss: 0.002\n",
      "Epoch 710 loss: 0.002\n",
      "Epoch 710 loss: 0.002\n",
      "Epoch 720 loss: 0.002\n",
      "Epoch 720 loss: 0.002\n",
      "Epoch 720 loss: 0.002\n",
      "Epoch 720 loss: 0.002\n",
      "Epoch 730 loss: 0.002\n",
      "Epoch 730 loss: 0.002\n",
      "Epoch 730 loss: 0.002\n",
      "Epoch 730 loss: 0.002\n",
      "Epoch 740 loss: 0.002\n",
      "Epoch 740 loss: 0.002\n",
      "Epoch 740 loss: 0.002\n",
      "Epoch 740 loss: 0.002\n",
      "Epoch 750 loss: 0.002\n",
      "Epoch 750 loss: 0.002\n",
      "Epoch 750 loss: 0.002\n",
      "Epoch 750 loss: 0.002\n",
      "Epoch 760 loss: 0.002\n",
      "Epoch 760 loss: 0.002\n",
      "Epoch 760 loss: 0.002\n",
      "Epoch 760 loss: 0.002\n",
      "Epoch 770 loss: 0.002\n",
      "Epoch 770 loss: 0.002\n",
      "Epoch 770 loss: 0.002\n",
      "Epoch 770 loss: 0.002\n",
      "Epoch 780 loss: 0.002\n",
      "Epoch 780 loss: 0.002\n",
      "Epoch 780 loss: 0.002\n",
      "Epoch 780 loss: 0.002\n",
      "Epoch 790 loss: 0.002\n",
      "Epoch 790 loss: 0.002\n",
      "Epoch 790 loss: 0.002\n",
      "Epoch 790 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 810 loss: 0.002\n",
      "Epoch 810 loss: 0.002\n",
      "Epoch 810 loss: 0.002\n",
      "Epoch 810 loss: 0.002\n",
      "Epoch 820 loss: 0.002\n",
      "Epoch 820 loss: 0.002\n",
      "Epoch 820 loss: 0.002\n",
      "Epoch 820 loss: 0.002\n",
      "Epoch 830 loss: 0.002\n",
      "Epoch 830 loss: 0.002\n",
      "Epoch 830 loss: 0.002\n",
      "Epoch 830 loss: 0.002\n",
      "Epoch 840 loss: 0.002\n",
      "Epoch 840 loss: 0.002\n",
      "Epoch 840 loss: 0.002\n",
      "Epoch 840 loss: 0.002\n",
      "Epoch 850 loss: 0.002\n",
      "Epoch 850 loss: 0.002\n",
      "Epoch 850 loss: 0.002\n",
      "Epoch 850 loss: 0.002\n",
      "Epoch 860 loss: 0.002\n",
      "Epoch 860 loss: 0.002\n",
      "Epoch 860 loss: 0.002\n",
      "Epoch 860 loss: 0.002\n",
      "Epoch 870 loss: 0.002\n",
      "Epoch 870 loss: 0.002\n",
      "Epoch 870 loss: 0.002\n",
      "Epoch 870 loss: 0.002\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 940 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# sigmoid激活函数\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#MSE损失函数\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred)**2).mean()\n",
    "\n",
    "class ourNeuralNet:\n",
    "    #初始化函数，这个网络有6个权重，3个偏置\n",
    "    def __init__(self):\n",
    "        #从正态分布中随机选值,初始化网络的6个权重和3个偏置\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "        \n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "        \n",
    "        #用于存储每次训练时，网络的损失值\n",
    "        self.trainnum = []\n",
    "        self.loss = []\n",
    "     #网络的前馈函数   \n",
    "    def feedforward(self,x):\n",
    "        h1 = sigmoid(self.w1*x[0]+self.w2*x[1]+self.b1)\n",
    "        h2 = sigmoid(self.w3*x[0]+self.w4*x[1]+self.b2)\n",
    "        o1 = sigmoid(self.w5*h1+self.w6*h2+self.b3)\n",
    "        return o1\n",
    "    \n",
    "    #训练函数，训练一个网络需要数据集，这里将数据集分成输入集data和值集all_y_trues\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1 #定义学习率为0.1\n",
    "        epochs = 1000 #在整个输入集上迭代1000次\n",
    "        loss = []\n",
    "        \n",
    "        #迭代1000次，其实这里可以用损失函数小于一个极小值来做判据更合适\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                #计算每个神经元的计算值和输出值\n",
    "                v_h1 = self.w1*x[0]+self.w2*x[1]+self.b1\n",
    "                h1 = sigmoid(v_h1)\n",
    "                \n",
    "                v_h2 = self.w3*x[0]+self.w4*x[1]+self.b2\n",
    "                h2 = sigmoid(v_h2)\n",
    "                \n",
    "                v_o1 = self.w5*h1+self.w6*h2+self.b3\n",
    "                o1 = sigmoid(v_o1)\n",
    "                y_pred = o1\n",
    "                \n",
    "                self.trainnum.append(epoch)\n",
    "                self.loss.append(mse_loss(y_true, y_pred))\n",
    "               \n",
    "                #利用上边推导的公式，计算输出值对各个参数和偏置的偏导\n",
    "                #由公式（4.3）\n",
    "                \n",
    "                #损失函数L对y_pred的偏导\n",
    "                d_L_d_ypred = -2*(y_true-y_pred)\n",
    "                \n",
    "                \n",
    "                d_o1_d_vo1 = o1*(1-o1)\n",
    "                \n",
    "                d_vo1_d_h1 = self.w5\n",
    "                d_vo1_d_h2 = self.w6\n",
    "                \n",
    "                \n",
    "                d_h1_d_w1 = h1*(1-h1)*x[0]\n",
    "                d_h1_d_w2 = h1*(1-h1)*x[1]\n",
    "                d_h1_d_b1 = h1*(1-h1)*1\n",
    "                                \n",
    "                d_h2_d_w3 = h2*(1-h2)*x[0]\n",
    "                d_h2_d_w4 = h2*(1-h2)*x[1]\n",
    "                d_h2_d_b2 = h2*(1-h2)*1\n",
    "                \n",
    "                d_vo1_d_w5 = h1\n",
    "                d_vo1_d_w6 = h2\n",
    "                d_vo1_d_b3 = 1\n",
    "                \n",
    "                #损失函数L对权重w1的偏导\n",
    "                d_L_d_w1 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_h1*d_h1_d_w1\n",
    "                #损失函数L对权重w2的偏导\n",
    "                d_L_d_w2 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_h1*d_h1_d_w2\n",
    "                #损失函数L对偏置b1的偏导\n",
    "                d_L_d_b1 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_h1\n",
    "                \n",
    "                #损失函数对偏置b2的偏导\n",
    "                d_L_d_b2 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_h2\n",
    "                #损失函数L对权重w3的偏导\n",
    "                d_L_d_w3 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_h2*d_h2_d_w3\n",
    "                #损失函数L对权重w4的偏导\n",
    "                d_L_d_w4 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_h2*d_h2_d_w4\n",
    "                \n",
    "                #损失函数对权重w5的偏导\n",
    "                d_L_d_w5 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_w5\n",
    "                #损失函数对权重w6的偏导\n",
    "                d_L_d_w6 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_w6\n",
    "                #损失函数对偏置b3的偏导\n",
    "                d_L_d_b3 = d_L_d_ypred*d_o1_d_vo1*d_vo1_d_b3\n",
    "                \n",
    "                self.w1 -= learn_rate*d_L_d_w1\n",
    "                self.w2 -= learn_rate*d_L_d_w2\n",
    "                self.w3 -= learn_rate*d_L_d_w3\n",
    "                self.w4 -= learn_rate*d_L_d_w4\n",
    "                self.w5 -= learn_rate*d_L_d_w5\n",
    "                self.w6 -= learn_rate*d_L_d_w6\n",
    "                self.b1 -= learn_rate*d_L_d_b1\n",
    "                self.b2 -= learn_rate*d_L_d_b2\n",
    "                self.b3 -= learn_rate*d_L_d_b3\n",
    "                \n",
    "                if epoch %10 == 0:\n",
    "                    y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                    loss = mse_loss(all_y_trues, y_preds)\n",
    "                    print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "   \n",
    "    #画出每次迭代的损失函数\n",
    "    def draw_loss_pic(self):\n",
    "        plt.plot(self.trainnum,self.loss)\n",
    "        plt.show()\n",
    "        \n",
    "data = np.array([[-2,-1],[25,6],[17,4],[-15,-6]])\n",
    "all_y_trues = np.array([1,0,0,1])\n",
    "#实例化这个网络\n",
    "my_neural_net = ourNeuralNet()\n",
    "#将实例输入网络，进行训练\n",
    "my_neural_net.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#画出损失函数随训练次数的参数\n",
    "my_neural_net.draw_loss_pic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上边的输出和图形可以看出，随着迭代的进行，损失函数的值不断降低。\n",
    "至此，我们基本实现了一个完整的神经网络。做一下简单的回顾：\n",
    "+ 神经元是神经网络的基本组成单元，神经元要做的事就是将输入乘以权重，加上偏置，代入激活函数并输出\n",
    " + 神经元的参数是权重和偏置，方法是激活函数\n",
    "+ 神经网络是由一个一个神经元相互连接而成，形成一层一层的结构，上一层的神经元的输出作为下一层神经元的输入\n",
    " + 神经网络的层数可以随意，每层的神经元的数量也可以随意\n",
    " + 复杂的神经网络与简单的神经网络的基本原理都是一样的，只不过是层数多一些，每层的神经元数量多一些\n",
    "+ 通过损失函数来评价一个神经网络的好坏，损失函数定义了神经网络的预测值与真实值之间的差距\n",
    " + 通过求损失函数对神经网络上各个参数和偏置的偏导，来更新参数\n",
    " + 常用的随机梯度下降法是指对每一个训练集中的样本，代入网络之后均进行一次网络参数的修正，然后使用修正后的参数计算下一个训练样本，如此循环\n",
    " + 学习率表示了参数更新的步长，不能过大也不能过小\n",
    "+ 训练神经网络需要在训练集上迭代很多次，直到损失函数值极小或者迭代到足够多的次数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
